# Toxicity Classification Models

This repository contains pre-trained models for toxicity classification in text comments.

## Quick Start Guide

### Prerequisites
1. Install Python 3.8 or higher
2. Install required packages:
   ```bash
   pip install torch transformers scikit-learn
   ```
## Note: pip install any other dependanceies your terminal says you're missing when trying to run

## Required Files

├── bert_toxicity_model/
│   ├── config.json
│   ├── model.safetensors        ## This is the import from the google drive folder
│   ├── tokenizer_config.json
│   ├── special_tokens_map.json
│   └── vocab.txt
├── char_vectorizer.pkl
├── word_vectorizer.pkl
├── logistic_model.pkl
├── test_bert_model.py
└── test_model.py

### Testing the Models
1. Download all files into the structure detailed above
2. Download files too large for github:
   - https://drive.google.com/file/d/12K1R2v9Zst_tuRE1jeRoGLiu3j4RDIS8/view?usp=sharing
3. Run either test script:
   ```bash
   python test_bert_model.py
   # or
   python test_model.py
   ```
4. Enter text when prompted to get toxicity predictions
5. Type 'exit' to quit

