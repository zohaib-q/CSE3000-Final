# Toxicity Classification Models

This repository contains pre-trained models for toxicity classification in text comments.

## Quick Start Guide

### Prerequisites
1. Install Python 3.8 or higher
2. Install required packages:
   ```bash
   pip install torch transformers scikit-learn
   ```
## Note: pip install any other dependanceies your terminal says you're missing when trying to run

### Testing the Models
1. Download all files into a single folder
2. Download files too large for github:
   - https://drive.google.com/file/d/12K1R2v9Zst_tuRE1jeRoGLiu3j4RDIS8/view?usp=sharing
3. Run either test script:
   ```bash
   python test_bert_model.py
   # or
   python test_model.py
   ```
4. Enter text when prompted to get toxicity predictions
5. Type 'exit' to quit

## Required Files
```
.
├── bert_toxicity_model/
│   ├── config.json
│   ├── model.safetensors        ## This is the important google drive folder
│   ├── tokenizer_config.json
│   ├── special_tokens_map.json
│   └── vocab.txt
├── char_vectorizer.pkl
├── word_vectorizer.pkl
├── logistic_model.pkl
├── test_bert_model.py
└── test_model.py
```

## Notes
- The BERT model requires more computational resources (will take 1 - 2 min to load)
- Both models will classify text as either "Toxic" or "Non-Toxic"
- Make sure all model files are in the same folder as the test scripts 
